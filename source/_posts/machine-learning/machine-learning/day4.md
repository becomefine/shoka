---
title: 决策树
date: 2022-05-30 21:31:22
categories:
 - [机器学习,机器学习]
tags: 
 - 机器学习
 - 机器学习算法
 - 周志华
---

# 决策树

## 决策树学习基本算法

:::default no-icon
**输入**: 训练集$D = {(\vec{x_{1}}, y_{1}),(\vec{x_{2}}, y_{2}),...,(\vec{x_{m}},y_{m})}$;
      属性集$A = {a_{1},a_{2},a_{3},...,a_{d}}$
**过程**: 函数TreeGenerate(D, A)
1: 生成结点node;
2: **if** D中样本全属于同一类别C **then**
3: &ensp 将node标记为*C*类叶节点；**return**
4: **end if**
5: **if $A = \emptyset$ OR** *D*中样本在*A*上取值相同**then**
6: &ensp 将node标记为叶节点，其类别标记为*D*中样本数最多的类；**return**
7: **end if**
8: 从*A*中选择最优划分属性$a_{*};$
9: **for** $a_{*}$每一个值$a_{*}^{v}$ **do**
10: &ensp 为node生成一个分支；另$D_{v}$表示$D$中在$a_{*}$上取值为$a_{*}^{v}$的样本子集；
11: &ensp **if** $D_{v}$ 为空 **then**
12: &emsp 将分支结点标记为叶节点，其类别标记为$D$中样本最多的类；**return**
13: &ensp **else**
14: &emsp 以TreeGenerate($D_{v}, A\\{a_{*}}$)
15: &ensp **else**
16: **end for**
**输出**：以node为根结点的一棵决策树。
:::

## 划分选择

信息增益
基尼系数:CART决策树使用，反映了从数据集$D$随机抽取两个样本，其类别标记不一致的概率。Gini(D)越小，则数据集$D$的纯度越高。
$$\begin{aligned}
Gini(D) &= \sum_{k=1}^{|y|}\sum_{k' \neq k}p_{k}p_{k'} \\&= 1 - \sum_{k=1}^{|y|}p_{k}^{2}
\end{aligned}$$

## 剪枝处理

### 预剪枝
降低了过拟合的风险，显著减少决策树的训练时间开销和测试时间开销。

### 后剪枝
保留更多分支，欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中非叶节点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

## 连续与缺失值

### 连续值处理

### 后剪枝

