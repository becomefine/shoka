---
title: 神经网络
math: true
date: 2022-05-05 17:49:29
categories:
 - [机器学习,机器学习算法]
tags: 
 - 机器学习
 - 机器学习算法
 - 数学算法
---

# 神经网络

常见的神经网络模型: 深度神经网络、卷积神经网络，以及由这些基本网络优化而形成的各种深度学习模型。

## 神经元

神经元是神经网络算法的基本单元，本质上是一种函数，接受外部刺激并根据输入产生对应的输出。内部结构可以看作是线性函数和激活函数的组合，线性函数运算结果传递给激活函数，最终产生该神经元的输出结果。

### 感知器

感知器也被称为感知机，可以被视为一种形式最简单的前馈式人工神经网络，是一种二元线性分类器。感知器接收多个二进制输入并产生一个二进制输出。
![machine](/assets/machine-learning/zero-book/network1.png)
工作原理：
1、感知器接收多个二进制输入，每个输入对应一个权重。
2、感知器二进制输入的加权值对输出有重大影响。
3、通过感知器加权值与阈值比较，决定最后的二进制输出值。

输出值 $= \begin{cases}0, & \Sigma w_{i} x_{i} \leqslant \text { 阈值 } \\ 1, & \Sigma w_{i} x_{i}>\text { 阈值 }\end{cases}$

### S型神经元

:::warning no-icon
感知器中权重或偏重的调整会导致最后输出结果的改变。包含感知器的神经网络并不能使得任何权重或偏置的微小变化，最后都会导致输出结果的微小变化。因为网络中某个感知器的权重或偏置发生的微小变化有时会引起感知器输出值的极大变化。
:::

S型神经元与感知器相比，其优点在于：权重和偏置的微小变化只会导致输出的微小变化。
与感知器最大区别在于它的输入和输出不再是二进制的离散值，而是0~1的连续值。
特点：
1、S型神经元有多个输入值，这些输入值为0~1的数值。
2、S型神经元输入的加权值经过sigmoid函数处理后，输出一个0~1的数值。
![machine](/assets/machine-learning/zero-book/sigmoid.png)
S型神经元与感知器的不同之处在于: S型神经元是一个平滑的函数，而感知器是一个阶跃函数。

## 典型神经网络多层感知器

### 神经网络结构

![machine](/assets/machine-learning/zero-book/network2.png)
1、输入层。输入层是神经网络的第一层，图像通过数值化转换输入该层，该层接收输入信号值并传递到下一层，对输入的信号值并不执行任何运算，没有自己的权重值和偏置值。
2、隐藏层。隐藏层是神经网络中介于输入层和输出层之间的合成层。一个神经网络包含一个或多个隐藏层，隐藏层的神经元通过层层转换，不断提高和已标注图像的整体相似度，最后一个隐藏层将值传递给输出层。
3、输出层。输出层是神经网络的最后一层，接收最后一个隐藏层的输入而产生最终的预测结果，得到理想范围内的期望数目的值。

### MLP

MLP是一种典型的神经网络结构。
![machine](/assets/machine-learning/zero-book/MLP.png)

手写数字神经网络算法主要环节：
1、图像数值化：将图像转化为数值
2、激活规则：神经元间如何相互影响
某个神经元激活值由上一层神经元激活值的某种加权方式来决定。例如，神经元$c_{1}$激活值由上一层神经元激活值的某种加权形式来表达，如$\sigma (w_{1}a_{1}+w_{2}a_{2}+···+w_{n}a_{n}+b)$，其中$a_{i}$是上一层某个神经元的激活值；$w_{i}$是上一层该神经元激活值对激活值对神经元$c_{i}$激活值影响的权重；$b$是神经元$c_{1}$被激活的难易程度，即偏置；$\sigma$是sigmoid函数，主要作用是将函数值压缩为0~1。
![machine](/assets/machine-learning/zero-book/sigmoid1.png)

- 权重。表示神经元之间联系的强度。
- 偏重。神经元输入信号总强度大并不意味着其总是能够处于激活状态，还需要考虑神经元阈值情况。只有神经元信号强度大于阈值，神经元才会被激活。常见情况是，将阈值的相反数定义为神经元激活规则的一部分，这部分称为偏置。偏置表示神经元是否容易被激活，且是一个可以不断调整的参数。
3、激活规则的关键：神经网络非线性矫正
早期神经网络的非线性函数常使用sigmoid函数来将数值压缩为0~1，现在更多使用relu或tanh函数来进行处理。
sigmoid函数和tanh函数亲缘关系较近，一般认为tanh函数是sigmoid函数的改造版本。在神经网络的隐藏层中，tanh函数的表现要优于sigmoid函数，因为tanh函数范围为-1~1，数据的平均值为0，有类似数据中心化的效果。
在神经网络的输出层中，sigmoid函数的表现要优于tanh函数，这是因为sigmoid函数输出结果为0~1，而tanh函数输出结果为-1~1.输出结果为0~1更符合人们的习惯认知。
relu函数不同于上述两个函数，在深层网络中使用较多。工程实践中，sigmoid函数和tanh函数会在深层网络训练中出现端值饱和的现象，从而导致网络训练速度变慢。因此，一般在神经网络层次较浅时使用sigmoid函数和tanh函数，而在深层网络中使用relu函数。

4、代价函数与参数优化
直观上，每个神经元都与上一层所有神经元相关联，加权公式中的权重就代表这种联系的强弱，而偏置则表明该神经元是否容易被激活。
根据代价函数和目标函数产生的反馈结果进行自我改进

## MLP的代价函数与梯度下降

MLP神经网络学习过程由信号的正向传播与误差的反向传播两个过程组成，这两个过程以神经网络输出值与实际值的差距最小化为目标，不断循环往复，最终求解到满足代价函数最小化的参数值。
1、正向传播时，输入样本从输入层传入，经各隐藏层逐层处理后传向输出层。若输出层的输出值与实际值不符，则转入误差的反向传播阶段。
2、误差的反向传播时将输出误差以某种形式通过隐藏层向输入层逐层反向传播，并将误差分摊给各层的所有单元，从而获得各层单元的误差信号，此误差信号即作为修正各单元权值的依据。
3、这个信号的正向传播与误差的反向传播的各层权值调整过程，是周而复始进行的。权值不断调整的过程，也就是神经网络的学习训练过程，此过程一直进行到神经网络输出的误差减少到可接受的程度或进行到预先设定的学习次数为止。

### 代价函数

典型形式，输出的预测值与真实值差的平方和，即$\sum(真实值_{i}-预测值_{i})^2$。当神经网络对图像分类正确时，代价函数的数值就小；当神经网络对图像分类错误时，代价函数就大。

![machine](/assets/machine-learning/zero-book/pricefunction.png)

### 梯度下降法：求解代价函数最小值
