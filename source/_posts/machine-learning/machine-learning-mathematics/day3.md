---
title: '凸优化核心过程: 梯度下降过程'
math: true
date: 2022-04-26 15:39:38
categories:
 - [机器学习,机器学习算法]
tags: 
 - 机器学习
 - 机器学习算法
 - 数学算法
---

# 凸优化核心过程：搞懂梯度下降过程

优化问题可以分为凸优化问题和非凸优化问题
定义：定义在目标函数极值的求解。
凸优化问题的局部最优解就是全局最优解

## 凸函数

凸函数是一个定义在某个向量空间的凸子集$C$(区间)上的实值函数$f$，而且对于凸子集$C$中的任意两个向量，如果$f\left(\left(x_{1}+x_{2}\right) / 2\right) \leqslant\left(f\left(x_{1}\right)+f\left(x_{2}\right)\right) / 2$，则$f(x)$是定义在凸子集$C$中的凸函数。

## 梯度

梯度实际上是多变量微分的一般化，例如$J(\theta)=3 \theta_{1}+4 \theta_{2}-5 \theta_{3}-1.2$。
求解微分得到梯度: $\nabla J(\theta)=\left\langle\frac{\partial J}{\partial \theta_{1}}, \frac{\partial J}{\partial \theta_{2}}, \frac{\partial J}{\partial \theta_{3}}\right\rangle=\langle 3,4,-5\rangle$
梯度本质是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大值，即该函数在该点处沿着该方向(此梯度的方向)变化最快，变化率(该梯度的模)最大。

1、在单变量函数中，梯度就是函数的导数，表示函数在某个给定点的切线斜率，表示单位自变量变化引起的因变量变化值。
2、在多变量函数中，梯度就是函数分别对每个变量进行微分的结果，表示函数在给定点上升最快的方向和单位自变量(每个自变量)变化引起的因变量变化值。

## 梯度下降与参数求解

梯度下降是一种求解凸函数极值的方法，它以损失函数作为纽带。损失函数是模型预测值与训练之数据真实值的差距，它是模型参数(如$\omega$ 和 $b$)的函数。
损失函数有时候指代的是训练集数据中单个样本的预测值与真实值的差距，有时候指代的则是整个训练集所有样本的预测值与真实值的差距(也称为成本函数)。

梯度下降法迭代结束的情景
1、设置阈值。设置一个迭代结束的阈值，当两次迭代结果小于该阈值时，迭代借宿。
2、设置迭代次数。设置一个迭代次数，当迭代次数达到设定值，迭代结束。

# 线性回归

![machine](/assets/machine-learning/zero-book/recur1.png)

1、选择算法
2、损失函数
3、参数估计
4、正则化

# 逻辑回归

![machine](/assets/machine-learning/zero-book/recur2.png)

1、选择算法
2、损失函数
3、参数估计
4、正则化

# 决策树

## 典型决策树

决策树通过训练数据构建一种类似于流程图的树结构来对问题进行判断。

## 决策树算法的关键

找到决策树的关键在于判断根节点的属性。选择根节点的原则就是其信息增益最大，也就是尽可能消除决策的不确定性。

# 支持向量机

## SVM作用

![machine](/assets/machine-learning/zero-book/svm1.png)

SVM根据线性可分的程度不同，可以分为3类：线性可分SVM、线性SVM和非线性SVM。
线性可分SVM是线性SVM的基础。

## SVM算法原理和过程

### 分离超平面

![machine](/assets/machine-learning/zero-book/svm2.png)

判断标准: 间隔和支持向量

### 间隔与支持向量

![machine](/assets/machine-learning/zero-book/svm3.png)

线性不可分如何处理？

![machine](/assets/machine-learning/zero-book/svm4.png)
现在流行的解决线性不可分的方法就是使用核函数。核函数解决线性不可分的本质思想就是把原始样本通过核函数映射到高维空间中，从而让样本在高维空间中成为线性可分的，然后使用常见的线性分类器进行分类。这里需要强调的是，核函数不是某一种函数，而是一类功能性函数，凡是能够完成高纬度映射功能的函数都可以作为核函数。


