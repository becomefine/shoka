---
title: 面向大数据分析的机器学习算法
math: true
date: 2022-05-14 11:32:40
categories:
 - [大数据,计算智能]
tags: 
 - 大数据
 - 计算智能
---

# 降维算法和其他相关算法

降维的本质是一个映射函数：
$$ f: x \rightarrow y $$

线性降维算法： 主从分析法、线性判别分析法
非线性降维算法：局部线性嵌入、等距特征映射

**降维算法列表**
|降维算法|基本思想|
|:------|:------|
|主成分分析法(PCA)|用几个综合指标(主成分)来代替原始数据中的所有指标|
|奇异值分析(SVD)|利用矩阵的奇异值分解，选择较大的奇异值而抛弃较小的奇异值来降低矩阵的维数|
|因子分析(FA)|通过分析数据的结构，发现各属性之间的内在联系，从而找出属性的共性(因子)|
|偏最小二乘法|集主成分分析、典型相关分析和多元线性回归分析3种分析方法的优点于一身，既可以降维，又可以预测|
|Sammon mapping|在保持点间距离结构的同时，将高维空间的数据映射到低维空间|
|判别分析(DA)|将具有类标签的高维空间的数据(点)投影到低维空间，使其在低维空间按类别划分|
|局部线性嵌入(LLE)|是一种非线性降维算法，能够使降维后的数据较好地保持原有流形结构|
|拉普拉斯特征映射|希望相互间有关系的点(在图中相连的点)在降维后的空间尽可能地靠近|

## 主成分分析法

1、主成分保留了原始变量等绝大多数信息。
2、主成分的个数大大少于原始变量的数目。
3、各主成分之间互不相关。
4、每个主成分都是原始变量的线性组合。

步骤：
1.数据标准化
2.求相关系数矩阵
3.求特征值和特征向量
4.计算主成分贡献率和累计贡献率
5.根据指定贡献率，确定主成分并输出

## 半监督学习和增强学习及表示学习

### 增强学习

需要制定一个可以关联预测模型的采取相应行动的策略。
最终目标是在有限的条件下达到均衡。

### 表示学习

自动、高效地提取特征：
1、监督学习从输入有标签的数据中提取特征，如人工神经网络、多层感知器和监督学习词典。
2、无监督学习从没有类标签的数据中提取特征，如字典学习、独立分量分析、自编码、矩阵分解和聚类分析。

### 半监督学习

不同半监督算法的三个假设
1、平滑性假设
2、聚类假设
3、流行假设

## 学习合适的机器学习算法

### 性能指标

1、算法性能指标
- 准确度
- 训练时间
- 线性度

2、数据预处理方法
- 处理缺失数据：去平均分或去除确实样本
- 处理不正确的数据：拟合或插值
- 规则化数据：归一化或标准化处理
- 可视化支持：降维

3、机器学习表现分

归一化到[0,1]

4、模型拟合情况

- 过拟合现象
- 欠拟合现象

### 避免过拟合现象

1、增加训练数据
2、特征筛选和降维
- 多项式拟合模型中降低多项式次数
- 神经网络模型中减少神经网络的层数和每层的节点数
- SVM中增加RBF-kernel的带宽

3、数据归一化
- L1正则化：对于最后的特征权重的影响是让特征获得的权重稀疏化，也就是对结果影响不大的特征，不给其赋予权重。
- L2正则化：对于最后的特征权重的影响是尽量打散权重到每个特征维度上，不让权重集中在某些维度上，出现权重特别高的特征。

### 避免欠拟合现象

原因：
1、数据集很少，训练过程和检验过程不能很好的执行
2、没有选择合适的机器学习算法。

解决方法：
1、改变模型参数
例如SVM模型的欠拟合现象：
- 改用人工神经网络(ANN)模型重新训练数据
- 改变SVM模型的核函数(将线性的改为非线性的)，再训练模型。

2、修正损失函数

![lose](/assets/big-data/cognitive-computing-introduction/loss.jpg)

损失函数代表着模型的预测和实际值之间的差距，损失函数的选择对于问题的解决和优化非常重要。
1) 0-1损失(zero-one loss)函数：直接对应分类问题中判断错的个数，是一个非凸函数，不实用。
2) hinge损失(hinge loss)函数：是SVM中使用到的，健壮性相对较高(对于异常点/噪声不敏感)的损失函数，但是它没有那么好的概率解释。
3) log损失(log-loss)函数：能非常好的表征概率分布。适合多分类场景，能知道结果属于每个类别的置信度。缺点是健壮性没有那么强，相比hinge损失函数会对噪声敏感一些。
4) 多项式损失(exponential loss)函数：在AdaBoost中用到的，对离群点/噪声非常敏感，对于boosting算法简单而有效。
5) 感知损失(perceptron loss)函数：这个函数可以看作是hinge损失函数的一个变种，hinge损失函数对于判定边界的点(正确端)惩罚力度很高。
优点：比hinge损失函数简单，缺点：不是max-margin boundary，得到的模型的泛化能力没有hinge损失函数强。

3、组合方法或者其他修正

利用多个同样的模型组合成一个模型来提高准确率。
例如：将提升(Ada-boost)思想运用到决策树中，提高决策树预测的准确率。

### 选择合适的算法

1、保持方法：将原始数据集分为两部分，一部分是训练集，一部分是验证集，模型在数据集上的表现就是模型在验证集上的准确率估计。
2、交叉验证：将原始数据集分为$k$个部分，依次选择一个部分作为验证集，其他部分作为训练集，模型在数据集上的表现就是模型在各验证集的准确率平均值。
3、自助法：在训练样本中有放回地随机抽样，抽取到的样本作为训练集，而没有选中的样本作为校验集，重复$k$次。模型在数据集上的表现就是模型在各验证集上准确率的加权均值。

